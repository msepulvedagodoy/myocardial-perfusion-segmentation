{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasSepulvedaGodoy/myocardial-perfusion-segmentation/blob/develop/training_on_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2cCiMcpN6xD"
      },
      "source": [
        "# **Training**\n",
        "\n",
        "This colab is for train the model with the myocardial perfusion and cine images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnvQASXNaiwE"
      },
      "source": [
        "We load libraries that we don't have in colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za0KOnH2aohF"
      },
      "outputs": [],
      "source": [
        "pip install pydicom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2DLkfiZODgo"
      },
      "source": [
        "We import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib7PHvaJHwV1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import nibabel as nib\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVkiTGpmKwzb"
      },
      "outputs": [],
      "source": [
        "a = pydicom.read_file('/content/drive/MyDrive/Tesis/images/myoperfdata/Study7_Rest/data010.dcm').pixel_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7fDWeJLLFmr"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8uqPyOWLMRd"
      },
      "outputs": [],
      "source": [
        "a = cv2.imread('/content/drive/MyDrive/Tesis/images/myoperfdata/Study7_Rest/data010.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md5QJraSLSCA"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Zv-Q52rwq-"
      },
      "source": [
        "## Load images with Pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC71mjIC3hwB"
      },
      "source": [
        "We create a pytorch Dataset class for load all the images:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2RfKor-8sMx"
      },
      "source": [
        "### Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRzyNIlz8rhH"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "        image = torch.from_numpy(sample)\n",
        "        image = torch.unsqueeze(image, dim=0)\n",
        "        return image\n",
        "\n",
        "class ClipNorm(object):\n",
        "\n",
        "    def __init__(self, lower=0, upper=99):\n",
        "      self.lower = lower\n",
        "      self.upper = upper\n",
        "    \n",
        "\n",
        "    def __call__(self, sample):\n",
        "\n",
        "      lower_clip, upper_clip = np.percentile(sample, (self.lower, self.upper))\n",
        "      clip_sample = sample.clip(min=lower_clip, max=upper_clip)\n",
        "      clip_sample = self.rescale_intensity(clip_sample, (0, 255))\n",
        "\n",
        "      return clip_sample\n",
        "\n",
        "    def rescale_intensity(self, tensor, out_range):\n",
        "\n",
        "      current_min = torch.min(tensor)\n",
        "      current_max = torch.max(tensor)\n",
        "\n",
        "      tensor = (tensor - current_min) / (current_max - current_min)\n",
        "      tensor = out_range[0] + tensor * (out_range[1] - out_range[0])\n",
        "\n",
        "      return tensor\n",
        "\n",
        "class ZeroPad(object):\n",
        "\n",
        "    def __init__(self, size=256):\n",
        "      self.size = size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "      if max(sample.size()[1], sample.size()[2]) > self.size:\n",
        "        raise ValueError(\"Data can't be padded to a smaller shape.\")\n",
        "      \n",
        "      h_pad = self.size - sample.size()[1]\n",
        "      w_pad = self.size - sample.size()[2]\n",
        "\n",
        "      padding_left = w_pad // 2\n",
        "      padding_right = w_pad - padding_left\n",
        "\n",
        "      padding_top = h_pad // 2\n",
        "      padding_bottom = h_pad - padding_top\n",
        " \n",
        "      padding = (padding_left, padding_right, padding_top, padding_bottom)\n",
        "\n",
        "      paddet_data = torch.nn.functional.pad(sample, padding)\n",
        "\n",
        "      return paddet_data  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrqGbvNT82-u"
      },
      "source": [
        "### Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqHVooizXCRy"
      },
      "outputs": [],
      "source": [
        "class MyocardialPerfusionDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, transform_img = torchvision.transforms.Compose([ToTensor(), ClipNorm(), ZeroPad()]), transform_mark = torchvision.transforms.Compose([ToTensor(), ZeroPad()])) -> None:\n",
        "       \n",
        "        self.root = root\n",
        "        self.transform_mark = transform_mark\n",
        "        self.transform_img = transform_img\n",
        "\n",
        "        # create a dictionary for images.\n",
        "        locations = list(sorted(os.listdir(os.path.join(self.root, 'dicom'))))\n",
        "        img_dict = []\n",
        "        for loc in locations:\n",
        "            marks_ = nib.load(os.path.join(self.root, 'marks', '%s.nii.gz' % loc)).get_fdata()\n",
        "            for index, img in enumerate(list(sorted(os.listdir(os.path.join(self.root, 'dicom', loc))))):\n",
        "                if list(np.unique(marks_[:,:,index])) == [0., 1., 2.]:\n",
        "                    dict_ = {'patient_id': loc, 'img_dir': os.path.join(self.root, 'dicom', loc, img), 'mark': marks_[:,:,index]}\n",
        "                    img_dict.append(dict_)\n",
        "        \n",
        "        self.img_dict = img_dict\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path_img = self.img_dict[idx]['img_dir']\n",
        "        dicom = pydicom.read_file(path_img).pixel_array.astype('int32')\n",
        "        mark = self.img_dict[idx]['mark']\n",
        "        #mark = nib.load(os.path.join(self.root, 'marks', '%s.nii.gz' % self.img_dict[idx]['patient_id'])).get_fdata()[:,:,self.img_dict[idx]['mark']]\n",
        "\n",
        "        if self.transform_img:\n",
        "            dicom = self.transform_img(dicom)\n",
        "\n",
        "        if self.transform_mark:\n",
        "            mark = self.transform_mark(mark)\n",
        "        \n",
        "        return dicom, mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klt_Ek8u3thw"
      },
      "source": [
        "## Create Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JeymfwpA5WN"
      },
      "outputs": [],
      "source": [
        "def loader(dataset, batch_size, num_workers=2, shuffle=True):\n",
        "\n",
        "    input_images = dataset\n",
        "\n",
        "    input_loader = torch.utils.data.DataLoader(dataset=input_images,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=shuffle,\n",
        "                                                num_workers=num_workers)\n",
        "\n",
        "    return input_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnBMKsfWmjHY"
      },
      "outputs": [],
      "source": [
        "path_train = '/content/drive/MyDrive/Tesis/images/P-UCH/'\n",
        "path_val = '/content/drive/MyDrive/Tesis/images/Prueba'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvPMS1Nvykh1"
      },
      "outputs": [],
      "source": [
        "df = MyocardialPerfusionDataset(path_train)\n",
        "df_2 = MyocardialPerfusionDataset(path_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUQ843197bJB"
      },
      "outputs": [],
      "source": [
        "cv2_imshow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajCE5lwcytAV"
      },
      "outputs": [],
      "source": [
        "img = np.asarray(df[215][0]).transpose((1,2,0))\n",
        "img = np.squeeze(img)\n",
        "mark = np.asarray(df[215][1]).transpose((1,2,0))\n",
        "mark = np.squeeze(mark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Prc4DM4019A"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img, cmap='gray')\n",
        "plt.scatter(mark[:, 0], mark[:, 1], s=10, marker='.', c='r')\n",
        "plt.pause(0.001)\n",
        "plt.figure()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxOOgpUSKEmH"
      },
      "outputs": [],
      "source": [
        "def show_landmarks(image, landmarks):\n",
        "    \"\"\"Show image with landmarks\"\"\"\n",
        "    plt.imshow(image)\n",
        "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6M3sIxv2iJ2"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "show_landmarks(img, mark)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf_kpI8e3OiD"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 12))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img, 'gray', interpolation='none')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img, 'gray', interpolation='none')\n",
        "plt.imshow(mark, 'jet', interpolation='none', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_LPbGGJjFnT"
      },
      "outputs": [],
      "source": [
        "training_loader = loader(MyocardialPerfusionDataset(path_train), 12)\n",
        "val_loader = loader(MyocardialPerfusionDataset(path_val), 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M38g1ynB3Pu"
      },
      "source": [
        "## Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASguk6dFB6eY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import OrderedDict\n",
        "\n",
        "class UNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels=1, output_channels=1, init_features=32) -> None:\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "\n",
        "        self.encoder_1 = UNet._block(input_channels, features, name='encoder_1')\n",
        "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder_2 = UNet._block(features, features*2, name='encoder_2')\n",
        "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder_3 = UNet._block(features*2, features*4, name='encoder_3')\n",
        "        self.pool_3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder_4 = UNet._block(features*4, features*8, name='encoder_4')\n",
        "        self.pool_4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features*8, features*16, name='bottleneck')\n",
        "\n",
        "        self.upconv_4 = torch.nn.ConvTranspose2d(features*16, features*8, kernel_size=2, stride=2)\n",
        "        self.decoder_4 = UNet._block((features*8)*2, features*8, name='decoder_4')\n",
        "        self.upconv_3 = torch.nn.ConvTranspose2d(features*8, features*4, kernel_size=2, stride=2)\n",
        "        self.decoder_3 = UNet._block((features*4)*2, features*4, name='decoder_3')\n",
        "        self.upconv_2 = torch.nn.ConvTranspose2d(features*4, features*2, kernel_size=2, stride=2)\n",
        "        self.decoder_2 = UNet._block((features*2)*2, features*2, name='decoder_2')\n",
        "        self.upconv_1 = torch.nn.ConvTranspose2d(features*2, features, kernel_size=2, stride=2)\n",
        "        self.decoder_1 = UNet._block(features*2, features, name='decoder_1')\n",
        "        self.conv = torch.nn.Conv2d(features, output_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, sample):\n",
        "        encoder_1 = self.encoder_1(sample)\n",
        "        encoder_2 = self.encoder_2(self.pool_1(encoder_1))\n",
        "        encoder_3 = self.encoder_3(self.pool_2(encoder_2))\n",
        "        encoder_4 = self.encoder_4(self.pool_3(encoder_3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool_4(encoder_4))\n",
        "\n",
        "        decoder_4 = self.upconv_4(bottleneck)\n",
        "        decoder_4 = torch.cat((decoder_4, encoder_4), dim=1)\n",
        "        decoder_4 = self.decoder_4(decoder_4)\n",
        "        decoder_3 = self.upconv_3(decoder_4)\n",
        "        decoder_3 = torch.cat((decoder_3, encoder_3), dim=1)\n",
        "        decoder_3 = self.decoder_3(decoder_3)\n",
        "        decoder_2 = self.upconv_2(decoder_3)\n",
        "        decoder_2 = torch.cat((decoder_2, encoder_2), dim=1)\n",
        "        decoder_2 = self.decoder_2(decoder_2)\n",
        "        decoder_1 = self.upconv_1(decoder_2)\n",
        "        decoder_1 = torch.cat((decoder_1, encoder_1), dim=1)\n",
        "        decoder_1 = self.decoder_1(decoder_1)\n",
        "\n",
        "        return torch.sigmoid(self.conv(decoder_1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return torch.nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        torch.nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", torch.nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", torch.nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        torch.nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", torch.nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", torch.nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a2RDXXkgyMj"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = 1.0\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \n",
        "        assert y_pred.size() == y_true.size()\n",
        "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
        "        y_true = y_true[:, 0].contiguous().view(-1)\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dsc = (2. * intersection + self.smooth) / (\n",
        "            y_pred.sum() + y_true.sum() + self.smooth\n",
        "        )\n",
        "        return 1. - dsc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV5oAWffCTuv"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvsj1GIBGIJq"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QopueYeBg-rI"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 99:\n",
        "            last_loss = running_loss / 100 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpbbN_PuhF-_"
      },
      "outputs": [],
      "source": [
        "model = UNet()\n",
        "# Optimizers specified in the torch.optim package\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "loss_fn = DiceLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-EcAk97iWqf"
      },
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/myocardial_{}'.format(timestamp))\n",
        "\n",
        "epoch_number = 0\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "    # We don't need gradients on to do reporting\n",
        "    model.train(False)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    for i, vdata in enumerate(val_loader):\n",
        "        vinputs, vlabels = vdata\n",
        "        voutputs = model(vinputs)\n",
        "        vloss = loss_fn(voutputs, vlabels)\n",
        "        running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "training.ipynb",
      "provenance": [],
      "mount_file_id": "1F6FuIGTssPXr28wtPryqmkBV-p7EZryq",
      "authorship_tag": "ABX9TyNIY5W4lRy3MkJ2Nvk7DZPu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}